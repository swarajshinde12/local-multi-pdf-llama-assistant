ğŸš€ Local Multi-PDF LLaMA Assistant
A Fully Offline RAG System Using LLaMA, FAISS, and Sentence Transformers
ğŸ§  Overview

Local Multi-PDF LLaMA Assistant is a 100% offline, privacy-focused RAG (Retrieval-Augmented Generation) system that:

Reads multiple PDFs

Splits them into chunks

Generates semantic embeddings

Performs similarity search with FAISS

Feeds retrieved context into a local LLaMA GGUF model

Answers questions like ChatGPT â€” but completely offline

This project demonstrates practical, industry-level AI engineering skills, including NLP pipelines, embedding models, vector databases, and local LLM inference.

ğŸŒŸ Key Features
ğŸ“š Multi-PDF Support

Automatically loads every PDF in the data/ folder.

âœ‚ï¸ Smart Text Chunking

Chunking with overlap for maximum context retention.

ğŸ” Semantic Embeddings

Using sentence-transformers/all-MiniLM-L6-v2.

âš¡ FAISS Vector Search

Fast similarity queries across thousands of chunks.

ğŸ¤– Local LLaMA GGUF Model

Runs entirely offline using llama-cpp-python.

No API keys.
No internet.
No privacy risk.

ğŸ’¬ ChatGPT-Style Streamlit UI

Chat bubbles

Message history

Dark mode

Sidebar showing indexed PDFs

Smooth UX

ğŸš€ GPU Acceleration (Optional)

Automatically uses CUDA if installed.

ğŸ§© Modular Architecture

Every component cleanly separated inside modules/.

ğŸ—ï¸ Architecture Diagram
PDFs â†’ Text Extraction â†’ Chunking â†’ Embeddings â†’ FAISS Search â†’ Top-K Context
                  â†“                                                â†‘
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ LLaMA GGUF Model â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“ Project Structure
local-multi-pdf-llama-assistant/
 â”£ modules/
 â”ƒ â”£ pdf_loader.py
 â”ƒ â”£ text_splitter.py
 â”ƒ â”£ embedder.py
 â”ƒ â”£ vector_store.py
 â”ƒ â”£ local_llm.py
 â”ƒ â”£ local_llm_gguf.py
 â”ƒ â”£ multi_pdf_loader.py
 â”ƒ â”— multi_rag.py
 â”£ data/
 â”£ models/
 â”£ app.py
 â”£ app_chat_gguf.py
 â”£ app_gguf.py
 â”£ README.md
 â”£ requirements.txt
 â”£ .gitignore

ğŸ”§ Installation
1ï¸âƒ£ Clone repo
git clone https://github.com/swarajshinde12/local-multi-pdf-llama-assistant
cd local-multi-pdf-llama-assistant

2ï¸âƒ£ Create virtual environment
python -m venv venv
.\venv\Scripts\activate

3ï¸âƒ£ Install requirements
pip install -r requirements.txt

4ï¸âƒ£ Add a GGUF model

Download any LLaMA or Mistral GGUF file (example):

Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf

Place it in:

models/llm.gguf

â–¶ï¸ Run the App
Chat Interface (recommended)
streamlit run app_chat_gguf.py

Basic RAG app
streamlit run app_gguf.py

ğŸ’¡ Example Query

User:

What does this document say about neural networks?

Assistant (local LLaMA):
Summarizes using retrieved chunks + LLM reasoning.

ğŸ¯ Why This Project Impresses Recruiters

This project shows you can:

âœ” Implement real RAG pipelines
âœ” Work with embeddings + FAISS
âœ” Run local LLMs with quantization
âœ” Build modular AI systems
âœ” Build clean UI apps
âœ” Handle multi-PDF knowledge bases
âœ” Optimize for GPU where possible

This is exactly what companies hiring ML/AI engineers look for.

ğŸ”® Future Enhancements

Add reranking (BGE-Reranker / ColBERT)

Add conversation memory

Show citations in responses

Improve UI animations

Add support for DOCX / TXT
